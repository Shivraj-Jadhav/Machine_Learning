{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNzo8E8/cjEYGeJpA9dVk7+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Importing Relevant Libraries"],"metadata":{"id":"WTYHLP0s2eb0"}},{"cell_type":"code","execution_count":9,"metadata":{"id":"JIrE5auw0yhz","executionInfo":{"status":"ok","timestamp":1696929872298,"user_tz":-330,"elapsed":2061,"user":{"displayName":"Shivraj Jadhav","userId":"03346347953720223995"}}},"outputs":[],"source":["import numpy as np\n","from sklearn import preprocessing\n","import tensorflow as tf"]},{"cell_type":"markdown","source":["#Reading the Data"],"metadata":{"id":"kRmVA_vS3Mfq"}},{"cell_type":"code","source":["raw_data= np.loadtxt('Audiobooks_data.csv', delimiter = ',')\n","unscaled_inputs_all = raw_data[:, 1:-1]\n","targets_all = raw_data[:,-1]"],"metadata":{"id":"BqQaR6OR22p1","executionInfo":{"status":"ok","timestamp":1696929259994,"user_tz":-330,"elapsed":380,"user":{"displayName":"Shivraj Jadhav","userId":"03346347953720223995"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["#Balance the dataset"],"metadata":{"id":"82lcyIf03byR"}},{"cell_type":"code","source":["# Count how many targets are 1 (meaning that the customer did convert)\n","num_one_targets = int(np.sum(targets_all))\n","\n","zero_targets_counter = 0\n","\n","# We want to create a \"balanced\" dataset, so we will have to remove some input/target pairs.\n","indices_to_remove = []\n","\n","# Once there are as many 0s as 1s, mark entries where the target is 0.\n","for i in range(targets_all.shape[0]):\n","    if targets_all[i] == 0:\n","        zero_targets_counter += 1\n","        if zero_targets_counter > num_one_targets:\n","            indices_to_remove.append(i)\n","\n","# Create two new variables, one that will contain the inputs, and one that will contain the targets.\n","# We delete all indices that we marked \"to remove\" in the loop above.\n","unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n","targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"],"metadata":{"id":"HYXxC4s33QE5","executionInfo":{"status":"ok","timestamp":1696929408650,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shivraj Jadhav","userId":"03346347953720223995"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["#Standardize the inputs"],"metadata":{"id":"h0WSauTA4Jds"}},{"cell_type":"code","source":["scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"],"metadata":{"id":"NyFV-eBs39_j","executionInfo":{"status":"ok","timestamp":1696929474084,"user_tz":-330,"elapsed":413,"user":{"displayName":"Shivraj Jadhav","userId":"03346347953720223995"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["#Shuffle the data"],"metadata":{"id":"hFhw7QlR4Qzk"}},{"cell_type":"code","source":["shuffled_indices = np.arange(scaled_inputs.shape[0])\n","np.random.shuffle(shuffled_indices)\n","\n","# Use the shuffled indices to shuffle the inputs and targets.\n","shuffled_inputs = scaled_inputs[shuffled_indices]\n","shuffled_targets = targets_equal_priors[shuffled_indices]"],"metadata":{"id":"0Aw8LIlf4OEW","executionInfo":{"status":"ok","timestamp":1696929519246,"user_tz":-330,"elapsed":415,"user":{"displayName":"Shivraj Jadhav","userId":"03346347953720223995"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["#Split the dataset into train, validation, and test"],"metadata":{"id":"buluZxa54kvB"}},{"cell_type":"code","source":["# Count the total number of samples\n","samples_count = shuffled_inputs.shape[0]\n","\n","# Count the samples in each subset, we want 80-10-10 distribution of training, validation, and test.\n","# Naturally, the numbers are integers.\n","train_samples_count = int(0.8 * samples_count)\n","validation_samples_count = int(0.1 * samples_count)\n","\n","# The 'test' dataset contains all remaining data.\n","test_samples_count = samples_count - train_samples_count - validation_samples_count\n","\n","# Create variables that record the inputs and targets for training\n","# In our shuffled dataset, they are the first \"train_samples_count\" observations\n","train_inputs = shuffled_inputs[:train_samples_count]\n","train_targets = shuffled_targets[:train_samples_count]\n","\n","# Create variables that record the inputs and targets for validation.\n","validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n","validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n","\n","\n","# Create variables that record the inputs and targets for test.\n","test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n","test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n","\n","print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n","print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n","print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cRegNpQq4ZA_","executionInfo":{"status":"ok","timestamp":1696929714263,"user_tz":-330,"elapsed":15,"user":{"displayName":"Shivraj Jadhav","userId":"03346347953720223995"}},"outputId":"75dffd1a-6350-499a-c3c8-ad61a333e14e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["1789.0 3579 0.49986029617211514\n","235.0 447 0.5257270693512305\n","213.0 448 0.47544642857142855\n"]}]},{"cell_type":"code","source":["# Save the three datasets in *.npz.\n","# In the next lesson, you will see that it is extremely valuable to name them in such a coherent way!\n","\n","np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n","np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n","np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"],"metadata":{"id":"byM-yK1b5Io2","executionInfo":{"status":"ok","timestamp":1696929765804,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shivraj Jadhav","userId":"03346347953720223995"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["#Loading Data"],"metadata":{"id":"MmFa35qw5zZZ"}},{"cell_type":"code","source":["# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets\n","npz = np.load('Audiobooks_data_train.npz')\n","\n","# we extract the inputs using the keyword under which we saved them\n","# to ensure that they are all floats, let's also take care of that\n","train_inputs = npz['inputs'].astype(np.float)\n","# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\n","train_targets = npz['targets'].astype(np.int)\n","\n","# we load the validation data in the temporary variable\n","npz = np.load('Audiobooks_data_validation.npz')\n","# we can load the inputs and the targets in the same line\n","validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n","\n","# we load the test data in the temporary variable\n","npz = np.load('Audiobooks_data_test.npz')\n","# we create 2 variables that will contain the test inputs and the test targets\n","test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bXbl-39y5VU5","executionInfo":{"status":"ok","timestamp":1696931235516,"user_tz":-330,"elapsed":383,"user":{"displayName":"Shivraj Jadhav","userId":"03346347953720223995"}},"outputId":"2549f23e-9a7b-4b32-c3be-35f20737e0dd"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-18-447d6e979645>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  train_inputs = npz['inputs'].astype(np.float)\n","<ipython-input-18-447d6e979645>:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  train_targets = npz['targets'].astype(np.int)\n","<ipython-input-18-447d6e979645>:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n","<ipython-input-18-447d6e979645>:13: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n","<ipython-input-18-447d6e979645>:18: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n","<ipython-input-18-447d6e979645>:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n"]}]},{"cell_type":"markdown","source":["#Model"],"metadata":{"id":"lFYtLveR9Ow-"}},{"cell_type":"code","source":["# Set the input and output sizes\n","input_size = 10\n","output_size = 2\n","# Use same hidden layer size for both hidden layers. Not a necessity.\n","hidden_layer_size = 50\n","\n","# define how the model will look like\n","model = tf.keras.Sequential([\n","    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n","    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n","    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n","    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n","    # the final layer is no different, we just make sure to activate it with softmax\n","    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n","])\n","\n","\n","### Choose the optimizer and the loss function\n","\n","# we define the optimizer we'd like to use,\n","# the loss function,\n","# and the metrics we are interested in obtaining at each iteration\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","### Training\n","# That's where we train the model we have built.\n","\n","# set the batch size\n","batch_size = 100\n","\n","# set a maximum number of training epochs\n","max_epochs = 100\n","\n","# set an early stopping mechanism\n","# let's set patience=2, to be a bit tolerant against random validation loss increases\n","early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n","\n","# fit the model\n","# note that this time the train, validation and test data are not iterable\n","model.fit(train_inputs, # train inputs\n","          train_targets, # train targets\n","          batch_size=batch_size, # batch size\n","          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n","          # callbacks are functions called by a task when a task is completed\n","          # task here is to check if val_loss is increasing\n","          callbacks=[early_stopping], # early stopping\n","          validation_data=(validation_inputs, validation_targets), # validation data\n","          verbose = 2 # making sure we get enough information about the training process\n","          )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gq6n6OAg9Lxv","executionInfo":{"status":"ok","timestamp":1696931247718,"user_tz":-330,"elapsed":9639,"user":{"displayName":"Shivraj Jadhav","userId":"03346347953720223995"}},"outputId":"32cb4963-0ec3-40c9-ec3f-c654706c8f65"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","36/36 - 1s - loss: 0.6179 - accuracy: 0.6387 - val_loss: 0.5387 - val_accuracy: 0.7226 - 639ms/epoch - 18ms/step\n","Epoch 2/100\n","36/36 - 0s - loss: 0.4924 - accuracy: 0.7488 - val_loss: 0.4649 - val_accuracy: 0.7405 - 62ms/epoch - 2ms/step\n","Epoch 3/100\n","36/36 - 0s - loss: 0.4297 - accuracy: 0.7846 - val_loss: 0.4212 - val_accuracy: 0.7517 - 79ms/epoch - 2ms/step\n","Epoch 4/100\n","36/36 - 0s - loss: 0.3944 - accuracy: 0.7941 - val_loss: 0.4029 - val_accuracy: 0.7875 - 80ms/epoch - 2ms/step\n","Epoch 5/100\n","36/36 - 0s - loss: 0.3768 - accuracy: 0.8030 - val_loss: 0.3904 - val_accuracy: 0.7875 - 80ms/epoch - 2ms/step\n","Epoch 6/100\n","36/36 - 0s - loss: 0.3635 - accuracy: 0.8089 - val_loss: 0.3817 - val_accuracy: 0.7875 - 70ms/epoch - 2ms/step\n","Epoch 7/100\n","36/36 - 0s - loss: 0.3557 - accuracy: 0.8134 - val_loss: 0.3810 - val_accuracy: 0.7919 - 63ms/epoch - 2ms/step\n","Epoch 8/100\n","36/36 - 0s - loss: 0.3500 - accuracy: 0.8069 - val_loss: 0.3747 - val_accuracy: 0.7875 - 62ms/epoch - 2ms/step\n","Epoch 9/100\n","36/36 - 0s - loss: 0.3463 - accuracy: 0.8111 - val_loss: 0.3739 - val_accuracy: 0.7942 - 83ms/epoch - 2ms/step\n","Epoch 10/100\n","36/36 - 0s - loss: 0.3417 - accuracy: 0.8164 - val_loss: 0.3696 - val_accuracy: 0.7897 - 62ms/epoch - 2ms/step\n","Epoch 11/100\n","36/36 - 0s - loss: 0.3395 - accuracy: 0.8159 - val_loss: 0.3673 - val_accuracy: 0.7987 - 68ms/epoch - 2ms/step\n","Epoch 12/100\n","36/36 - 0s - loss: 0.3335 - accuracy: 0.8184 - val_loss: 0.3705 - val_accuracy: 0.7987 - 62ms/epoch - 2ms/step\n","Epoch 13/100\n","36/36 - 0s - loss: 0.3305 - accuracy: 0.8315 - val_loss: 0.3617 - val_accuracy: 0.7987 - 78ms/epoch - 2ms/step\n","Epoch 14/100\n","36/36 - 0s - loss: 0.3293 - accuracy: 0.8212 - val_loss: 0.3565 - val_accuracy: 0.7987 - 65ms/epoch - 2ms/step\n","Epoch 15/100\n","36/36 - 0s - loss: 0.3265 - accuracy: 0.8217 - val_loss: 0.3554 - val_accuracy: 0.8098 - 82ms/epoch - 2ms/step\n","Epoch 16/100\n","36/36 - 0s - loss: 0.3272 - accuracy: 0.8195 - val_loss: 0.3655 - val_accuracy: 0.7942 - 66ms/epoch - 2ms/step\n","Epoch 17/100\n","36/36 - 0s - loss: 0.3249 - accuracy: 0.8215 - val_loss: 0.3555 - val_accuracy: 0.8121 - 79ms/epoch - 2ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7e4e3508a4d0>"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["#Testing the Model"],"metadata":{"id":"eN04zvE5-qUA"}},{"cell_type":"code","source":["test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ohZwewQA-DJY","executionInfo":{"status":"ok","timestamp":1696931257032,"user_tz":-330,"elapsed":2901,"user":{"displayName":"Shivraj Jadhav","userId":"03346347953720223995"}},"outputId":"0601f535-6c55-4161-e368-fc59172b2950"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["14/14 [==============================] - 0s 1ms/step - loss: 0.3372 - accuracy: 0.8304\n"]}]},{"cell_type":"code","source":["print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFLgYpYF-0Vr","executionInfo":{"status":"ok","timestamp":1696931258656,"user_tz":-330,"elapsed":12,"user":{"displayName":"Shivraj Jadhav","userId":"03346347953720223995"}},"outputId":"b70f54cf-7746-4e35-dbb9-cb830446b817"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Test loss: 0.34. Test accuracy: 83.04%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lH5gaPoG-251"},"execution_count":null,"outputs":[]}]}